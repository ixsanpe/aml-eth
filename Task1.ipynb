{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Age estimation regression based on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import collections \n",
    "import datetime \n",
    "from datetime import date\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing, datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFECV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import IsolationForest, AdaBoostRegressor, ExtraTreesRegressor\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#import missingno as msno  \n",
    "from playsound import playsound\n",
    "from IPython.display import Audio\n",
    "import pygame\n",
    "\n",
    "def notification():\n",
    "    sound_file = 'sound.mp3'\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(sound_file)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    '''\n",
    "Plot training deviance\n",
    "''' \n",
    "def plotDeviance():\n",
    "    test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "    for i, pred in enumerate(regr.staged_predict(X_test)):\n",
    "        test_score[i] = regr.loss_(pred,y_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.title(\"Deviance\")\n",
    "    plt.plot(\n",
    "        np.arange(params[\"n_estimators\"]) + 1,\n",
    "        regr.train_score_,\n",
    "        \"b-\",\n",
    "        label=\"Training Set Deviance\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Boosting Iterations\")\n",
    "    plt.ylabel(\"Deviance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(test_score.mean())\n",
    "    mse = mean_squared_error(y_test, regr.predict(X_test))\n",
    "    print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "def scaler(X_train):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv').drop(columns = ['id'])\n",
    "y_train = pd.read_csv('y_train.csv').drop(columns = ['id'])\n",
    "X_test_h = pd.read_csv('X_test.csv').drop(columns = ['id'])\n",
    "#X_train = X_train.fillna(X_train.mean())\n",
    "#msno.matrix(X_train)\n",
    "#msno.heatmap(X_train)\n",
    "\n",
    "#X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "#X_train = X_train[relevant_features]\n",
    "\n",
    "\n",
    "#print(min(X_train.var()), max(X_train.var()))\n",
    "#print(X_train.var()[X_train.var()>8000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Filling missing Values\n",
    "'''\n",
    "\n",
    "#imp = KNNImputer(n_neighbors=200, weights=\"distance\")\n",
    "#imp = MissForest()\n",
    "#imp = IterativeImputer(max_iter=10, random_state=1, n_nearest_features = 100, verbose = 2)\n",
    "#imp = BiScaler()\n",
    "#X_train = imp.fit_transform(np.array(X_train))\n",
    "imp = IterativeImputer(max_iter=10, random_state=1, n_nearest_features = 20, verbose = 0)\n",
    "X_train = imp.fit_transform(np.array(X_train))\n",
    "print('Imputation Done')\n",
    "#X_train = pd.DataFrame(imp.transform(np.array(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Removed, resulting shape: (1208, 832) (1208, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "clustering = DBSCAN(eps=0.001, min_samples=3).fit(X_train)\n",
    "mask = clustering.labels_ != -1\n",
    "X_train, y_train =  np.array(X_train)[mask, :], np.array(y_train)[mask]\n",
    "'''\n",
    "\n",
    "clf = IsolationForest()\n",
    "outliers = clf.fit_predict(X_train)\n",
    "mask = outliers != -1\n",
    "X_train, y_train =  np.array(X_train)[mask, :], np.array(y_train)[mask]\n",
    "\n",
    "\n",
    "print('Outliers Removed, resulting shape:', X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection Done\n",
      "Shape after feature selection: (1208, 80)\n"
     ]
    }
   ],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#\n",
    "\n",
    "#X_train = scaler(X_train)\n",
    "\n",
    "#\n",
    "#scaler = Normalizer().fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# PCA: unsupervised (se queda con 828 features)\n",
    "    # print('shape before PCA is ', X_train.shape)\n",
    "    # selector = PCA(n_components='mle', whiten=False, svd_solver='full')\n",
    "    #selector.fit(X_train)  \n",
    "    #X_train = selector.transform(X_train)\n",
    "    #scalerPost = StandardScaler().fit(X_train)\n",
    "    #X_train = scalerPost.inverse_transform(X_train)\n",
    "    #print (X_train.shape)\n",
    "\n",
    "\n",
    "# Create and fit selector (univariate feature selection)\n",
    "k0=80\n",
    "selector = SelectKBest(mutual_info_regression, k=k0)\n",
    "selector.fit(X_train,  np.ravel(y_train))\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test_h = pd.DataFrame(X_test_h)\n",
    "#\n",
    "#\n",
    "X_train = X_train.iloc[:,cols]\n",
    "X_test_h = X_test_h.iloc[:,cols]\n",
    "\n",
    "\n",
    "#X_train = SelectKBest(f_regression, k=k0).fit_transform(X_train, np.ravel(y_train))\n",
    "\n",
    "\n",
    "\n",
    "print('Feature Selection Done')\n",
    "print('Shape after feature selection:', X_train.shape)\n",
    "notification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols2 = selector.ranking_==1\n",
    "cols2 = np.loadtxt('ranking200.txt') == 1\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test_h = pd.DataFrame(X_test_h)\n",
    "#\n",
    "#\n",
    "X_train = X_train.iloc[:,cols2]\n",
    "X_test_h = X_test_h.iloc[:,cols2]\n",
    "#X_train = RFECV(f_regression, k=200).fit_transform(X_train, np.ravel(y_train))\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Removed, resulting shape: (1159, 80) (1159, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization, Data split, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData Augmentation\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Normalization\n",
    "'''\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#\n",
    "#scaler = Normalizer().fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_train = scaler(X_train)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Data Split\n",
    "'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=1, shuffle = True)\n",
    "X_train = np.array(X_train)\n",
    "#X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Data Augmentation\n",
    "'''\n",
    "\n",
    "#print(pd.DataFrame(X_train).columns)\n",
    "#concat_data = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis = 1)\n",
    "\n",
    "\n",
    "#concat = np.concatenate((X_train, y_train), axis = 1)\n",
    "#concat = pd.DataFrame(concat)\n",
    "#concat.columns = concat.columns.astype(str)\n",
    "\n",
    "#concat = concat.rename(columns={concat.columns[-1]: \"y\"})\n",
    "\n",
    "#import smogn\n",
    "\n",
    "#housing_smogn = smogn.smoter(data = concat,y = concat.columns[-1])\n",
    "\n",
    "#X_train = pd.DataFrame(housing_smogn[housing_smogn.columns[:-1]])\n",
    "#y_train = pd.DataFrame(housing_smogn[housing_smogn.columns[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV Scores: [0.71370931 0.63070532 0.60032783 0.59324576 0.65819184]\n",
      "CV Score : Mean - 0.639236 | Std - 0.0438345 | Max - 0.7137093 | Min - 0.5932458\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training\n",
    "'''\n",
    "\n",
    "\n",
    "paramsH = {#\"random_state\": 1, \n",
    "\"learning_rate\": 0.1, \n",
    "\"l2_regularization\": 1.5,\n",
    "#\"max_iter\": 1000,\n",
    "\"max_depth\": 3,\n",
    "#\"scoring\": 'f1_micro',\n",
    "}\n",
    "\n",
    "paramsRF_random = {'n_estimators': 1000,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 4,\n",
    " 'max_features': 'auto',\n",
    " 'max_depth': 43,\n",
    " 'bootstrap': True}\n",
    "\n",
    "params = {\"random_state\": 1, \n",
    "\"learning_rate\": 0.1, \n",
    "\"n_estimators\": 3000, \n",
    "\"max_depth\": 5,\n",
    "\"max_features\":'sqrt',\n",
    "\"warm_start\":True\n",
    "}\n",
    "\n",
    "\n",
    "#modelfit(xgb1, train, predictors)\n",
    "\n",
    "#regr = GradientBoostingRegressor(random_state=1, loss = 'squared_error', learning_rate = 0.1, n_estimators=1000, max_depth= 3)\n",
    "regr = GradientBoostingRegressor(**params)\n",
    "#regr = HistGradientBoostingRegressor(**paramsH)\n",
    "\n",
    "#regr = AdaBoostRegressor(base_estimator=None,random_state=1, loss = 'square', learning_rate = 0.1, n_estimators=1000)\n",
    "#regr = RandomForestRegressor(n_estimators = 1000, verbose=0, n_jobs=-1)\n",
    "#regr = RandomForestRegressor(**paramsRF_random)\n",
    "\n",
    "#regr = AdaBoostClassifier(n_estimators=1000)\n",
    "#regr = MLPRegressor(hidden_layer_sizes=([512, 256, 128, 64]), random_state=1, max_iter=2000, verbose = False, learning_rate_init=0.001, early_stopping=True)\n",
    "scores = cross_val_score(regr, X_train, np.ravel(y_train), cv=5, scoring='r2')\n",
    "print('Total CV Scores:',scores)\n",
    "print(\"CV Score : Mean - %.7g | Std - %.7g | Max - %.7g | Min - %.7g\" % (np.mean(scores),np.std(scores),np.max(scores),np.min(scores)))\n",
    "        \n",
    "'''\n",
    "Performance\n",
    "'''\n",
    "\n",
    "#reg = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "regr.fit(X_train, np.ravel(y_train))\n",
    "#pred = reg.predict(X_test)\n",
    "#X_test = xgb.DMatrix(X_test)\n",
    "#pred = regr.predict(X_test)\n",
    "#print(min(pred), max(pred))\n",
    "#pygame.mixer.init()\n",
    "#pygame.mixer.music.load(sound_file)\n",
    "#pygame.mixer.music.play()\n",
    "#print(\"FINAL SCORE\", r2_score(y_test, pred))\n",
    "\n",
    "notification()\n",
    "\n",
    "#plotDeviance()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2.0: without data split (XGBoost Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV Scores: [0.42896075 0.52009719 0.56036437 0.58020332 0.61474382]\n",
      "CV Score : Mean - 0.5408739 | Std - 0.06378036 | Max - 0.6147438 | Min - 0.4289608\n",
      "FINAL SCORE 0.5103752346626063\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training without data split\n",
    "'''\n",
    "regr = XGBRegressor(learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=9,\n",
    "    min_child_weight=3,\n",
    "    min_split_loss=0.2,\n",
    "    subsample=0.9,\n",
    "    sampling_method = 'uniform', #uniform + subsample >= 0.5\n",
    "    colsample_bytree=0.7, #=subsample,\n",
    "    objective= 'reg:squarederror',\n",
    "    #nthread=4,\n",
    "    #scale_pos_weight=1,\n",
    "    seed=0,\n",
    "    tree_method = 'auto',\n",
    "    eval_metric = 'rmse',\n",
    "    reg_alpha='0.1'\n",
    "    )\n",
    "regr = XGBRegressor(learning_rate =0.1,\n",
    "    n_estimators=3000,\n",
    "    max_depth=9,\n",
    "    min_child_weight=3,\n",
    "    min_split_loss=0.2,\n",
    "    subsample=0.9,\n",
    "    sampling_method = 'uniform', #uniform + subsample >= 0.5\n",
    "    colsample_bytree=0.7, #=subsample,\n",
    "    objective= 'reg:squarederror',\n",
    "    #nthread=4,\n",
    "    #scale_pos_weight=1,\n",
    "    seed=0,\n",
    "    tree_method = 'auto',\n",
    "    eval_metric = 'rmse',\n",
    "    reg_alpha='0.1'\n",
    "    )\n",
    "#modelfit(xgb1, train, predictors)\n",
    "\n",
    "#regr = GradientBoostingRegressor(random_state=1, loss = 'squared_error', learning_rate = 0.1, n_estimators=1000, max_depth= 3)\n",
    "#regr = GradientBoostingRegressor(**params)\n",
    "#regr = HistGradientBoostingRegressor(**paramsH)\n",
    "\n",
    "#regr = AdaBoostRegressor(base_estimator=None,random_state=1, loss = 'square', learning_rate = 0.1, n_estimators=1000)\n",
    "#regr = RandomForestRegressor(n_estimators = 1000, verbose=0, n_jobs=-1)\n",
    "#regr = RandomForestRegressor(**paramsRF_random)\n",
    "\n",
    "#regr = AdaBoostClassifier(n_estimators=1000)\n",
    "#regr = MLPRegressor(hidden_layer_sizes=([512, 256, 128, 64]), random_state=1, max_iter=2000, verbose = False, learning_rate_init=0.001, early_stopping=True)\n",
    "scores = cross_val_score(regr, X_train, np.ravel(y_train), cv=5, scoring='r2')\n",
    "print('Total CV Scores:',scores)\n",
    "print(\"CV Score : Mean - %.7g | Std - %.7g | Max - %.7g | Min - %.7g\" % (np.mean(scores),np.std(scores),np.max(scores),np.min(scores)))\n",
    "        \n",
    "'''\n",
    "Performance\n",
    "'''\n",
    "\n",
    "#reg = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "regr.fit(X_train, np.ravel(y_train))\n",
    "pred = regr.predict(np.float32(X_test))\n",
    "#pred = regr.predict(X_test)\n",
    "#print(min(pred), max(pred))\n",
    "#pygame.mixer.init()\n",
    "#pygame.mixer.music.load(sound_file)\n",
    "#pygame.mixer.music.play()\n",
    "print(\"FINAL SCORE\", r2_score(y_test, pred))\n",
    "\n",
    "notification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.89493102608006 86.48822668857551\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('X_test.csv')\n",
    "X_test_h = np.array(test_data.loc[:, test_data.columns != 'id'])\n",
    "\n",
    "#X_test_h = scaler(X_test_h)\n",
    "imp = IterativeImputer(max_iter=10, random_state=1, n_nearest_features = 20, verbose = 0)\n",
    "X_test_h = imp.fit_transform(np.array(X_test_h))\n",
    "X_test_h = pd.DataFrame(imp.transform(X_test_h))\n",
    "\n",
    "X_test_h = X_test_h.iloc[:,cols]\n",
    "#X_test_h = scaler(X_test_h)\n",
    "\n",
    "\n",
    "pred_test = test_data.loc[:,['id']]\n",
    "\n",
    "#X_test_h = xgb.DMatrix(X_test_h)\n",
    "predictions = pred = regr.predict(np.float32(X_test_h))\n",
    "#predictions = pred =  regr.predict(X_test_h)\n",
    "pred_test['y'] = predictions\n",
    "\n",
    "print(min(pred_test['y']), max(pred_test['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.to_csv('outputXGB5-warmstateGB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test.to_csv('results' + str(datetime.date.today().strftime(\"%m/%d/%Y, %H:%M:%S\")) + '.csv',index=False)\n",
    "pred_test.to_csv('output '+ str(datetime.datetime.now()) + '.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Hyperparameter tuning of Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gradient Boosting hyperparameter tuning\n",
    "'''\n",
    "\n",
    "# First: Random search with CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# TREE PARAM\n",
    "    # Minimum number of samples required at each leaf node (terminal)\n",
    "        # it should be small for imbalanced classes\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Minimum number of samples required to split a node: \n",
    "#   too low tends to overfit, too high tends to underfit\n",
    "min_samples_split = [5, 8, 12]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 60] #up to 30,40% of total features\n",
    "\n",
    "# Maximum number of levels in tree: \n",
    "#   higher depth, higher overfit (learn specific relations)\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 4)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# BOOSTING PARAM\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "\n",
    "# Learning rate\n",
    "#   smaller values will make the model robust\n",
    "\n",
    "# Number of trees \n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 5000, num = 10)]\n",
    "\n",
    "# Subsample: fraction of observations to be selected for each tree\n",
    "#   if < 1.0 tends to a reduction of variance, increase in bias\n",
    "subsample = 0.8\n",
    "n_iter_no_change = 100\n",
    "validation_fraction = 0.1\n",
    "\n",
    "# MISCELLANEOUS PARAM\n",
    "loss = 'squared_error' #default works well\n",
    "random_state = None # we could define it\n",
    "warm_start = False #if True it calls the previous call to fit and add more estimators to the ensemble\n",
    "\n",
    "'''\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "'max_features': max_features,\n",
    "'max_depth': max_depth,\n",
    "'min_samples_split': min_samples_split,\n",
    "'min_samples_leaf': min_samples_leaf,\n",
    "'bootstrap': bootstrap}\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "XGB Boosting hyperparameter tuning\n",
    "'''\n",
    "\n",
    "regr = XGBRegressor(learning_rate =0.05,\n",
    "    n_estimators=2000,\n",
    "    max_depth=9,\n",
    "    min_child_weight=3,\n",
    "    min_split_loss=0.2,\n",
    "    subsample=0.59,\n",
    "    sampling_method = 'uniform', #uniform + subsample >= 0.5\n",
    "    colsample_bytree=0.7, #=subsample,\n",
    "    objective= 'reg:squarederror',\n",
    "    nthread=4,\n",
    "\n",
    "    #scale_pos_weight=1,\n",
    "    seed=0,\n",
    "    tree_method = 'auto',\n",
    "    eval_metric = 'rmse'\n",
    "    )\n",
    "\n",
    "param_test = {'reg_alpha':[1e-5, 1e-2,0, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=regr, param_grid=param_test,\n",
    "                              scoring='r2', \n",
    "                              cv = 10, verbose=2,  n_jobs=-1)\n",
    "\n",
    "gsearch1.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)\n",
    "notification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Plot of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "def modelfit(regr, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=10):\n",
    "    #Fit the algorithm on the data\n",
    "    regr.fit(X_train, np.ravel(y_train))\n",
    "        \n",
    "    #Predict training set:\n",
    "    pred = regr.predict(X_test)\n",
    "    #dtrain_predprob = regr.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(regr,  X_train, np.ravel(y_train), cv=cv_folds, scoring='r2')\n",
    "\n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy R2 : %.4g\" % (r2_score(y_test, pred)) )\n",
    "    #print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        print('Total CV Scores:',scores)\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(regr.feature_importances_).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "\n",
    "gbm0 = GradientBoostingRegressor(random_state=10, n_estimators = 600, max_features='sqrt')\n",
    "modelfit(gbm0, X_train, np.ravel(y_train))\n",
    "\n",
    "print(\"First gridsearch\")\n",
    "# Define parameters\n",
    "min_samples_leaf = 4\n",
    "min_samples_split = 12\n",
    "max_features = 'sqrt' #60\n",
    "max_depth = 6\n",
    "subsample = 0.8\n",
    "learning_rate = 0.1\n",
    "random_state = 10\n",
    "\n",
    "#n_estimators = [int(x) for x in np.linspace(start = 20, stop = 2000, num = 10)]\n",
    "\n",
    "\n",
    "params1 = {#'n_estimators': n_estimators,\n",
    "'max_features': max_features,\n",
    "'max_depth': max_depth,\n",
    "'min_samples_split': min_samples_split,\n",
    "'min_samples_leaf': min_samples_leaf,\n",
    "'subsample': subsample,\n",
    "'learning_rate': learning_rate,\n",
    "'random_state': random_state }\n",
    "\n",
    "param_test1 = {'n_estimators':range(100,3500,500)}\n",
    "gbm1 = GradientBoostingRegressor(**params1)\n",
    "\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(**params1)\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=gbm1, param_grid=param_test1,\n",
    "                              scoring='r2', \n",
    "                              cv = 10, verbose=2,  n_jobs=-1)\n",
    "gsearch1.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: random forest hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest hyperparameter\n",
    "'''\n",
    "\n",
    "# First: Random search with CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 60] #up to 30,40% of total features\n",
    "# Maximum number of levels in tree: higher depth, higher overfit (learn specific relations)\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 4)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node: \n",
    "#   too low tends to overfit, too high tends to underfit\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node (terminal)\n",
    "    # it should be small for imbalanced classes\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Grid search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {  'bootstrap': [True],\n",
    " 'max_depth': [5, 10, None],\n",
    " 'max_features': ['auto', 'log2'],\n",
    " 'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]\n",
    "  \n",
    "  }\n",
    "\n",
    " # Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, np.ravel(y_train))\n",
    "grid_search.best_params_\n",
    "\n",
    "best_gridRF = grid_search.best_estimator_ #model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handing in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('X_test.csv')\n",
    "X_test_h = np.array(test_data.loc[:, test_data.columns != 'id'])\n",
    "\n",
    "imp = IterativeImputer(max_iter=10, random_state=1, n_nearest_features = 10, verbose = 0)\n",
    "X_test_h = imp.fit_transform(np.array(X_test_h))\n",
    "X_test_h = pd.DataFrame(imp.transform(X_test_h))\n",
    "\n",
    "X_test_h = X_test_h.iloc[:,cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred_test = test_data.loc[:,['id']]\n",
    "predictions = pred = regr.predict(X_test_h)\n",
    "pred_test['y'] = predictions\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "722a6f6468c40eb10e080dc076ad1be2feff99a7f31db43232e41a319206c6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
